C:\Users\jonas\Desktop\DTU\MLOps\my_mlops\src\session2\data.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  train_images = torch.load("data/processed/train_images.pt")
Traceback (most recent call last):
  File "C:\Users\jonas\Desktop\DTU\MLOps\MLOps\session2repo\src\session2\train.py", line 70, in <module>
    typer.run(train)
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\typer\main.py", line 1074, in run
    app()
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\typer\main.py", line 340, in __call__
    raise e
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\typer\main.py", line 323, in __call__
    return get_command(self)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\click\core.py", line 1161, in __call__
    return self.main(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\typer\core.py", line 680, in main
    return _main(
           ^^^^^^
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\typer\core.py", line 198, in _main
    rv = self.invoke(ctx)
         ^^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\click\core.py", line 1443, in invoke
    return ctx.invoke(self.callback, **ctx.params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\click\core.py", line 788, in invoke
    return __callback(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\typer\main.py", line 698, in wrapper
    return callback(**use_params)
           ^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\DTU\MLOps\MLOps\session2repo\src\session2\train.py", line 35, in train
    train_set, _ = corrupt_mnist()
                   ^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\DTU\MLOps\my_mlops\src\session2\data.py", line 38, in corrupt_mnist
    train_images = torch.load("data/processed/train_images.pt")
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\torch\serialization.py", line 1319, in load
    with _open_file_like(f, "rb") as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\torch\serialization.py", line 659, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\torch\serialization.py", line 640, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'data/processed/train_images.pt'
[31m╭─[0m[31m─────────────────────────────────────────────────────────────────[0m[31m [0m[1;31mTraceback [0m[1;2;31m(most recent call last)[0m[31m [0m[31m─────────────────────────────────────────────────────────────────[0m[31m─╮[0m
[31m│[0m [33mC:\Users\jonas\Desktop\DTU\MLOps\MLOps\session2repo\src\session2\train.py[0m:[94m35[0m in [92mtrain[0m                                                                                 [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m   [2m32 [0m[2m│   │   [0mconfig={[33m"[0m[33mlr[0m[33m"[0m: cfg.optimizer[[33m'[0m[33mlr[0m[33m'[0m], [33m"[0m[33mbatch_size[0m[33m"[0m: hparams[[33m'[0m[33mbatch_size[0m[33m'[0m], [33m"[0m[33mepochs[0m[33m"[0m                                                                         [31m│[0m
[31m│[0m   [2m33 [0m[2m│   [0m)                                                                                                                                                            [31m│[0m
[31m│[0m   [2m34 [0m[2m│   [0m                                                                                                                                                             [31m│[0m
[31m│[0m [31m❱ [0m35 [2m│   [0mtrain_set, _ = [1;4mcorrupt_mnist()[0m                                                                                                                               [31m│[0m
[31m│[0m   [2m36 [0m[2m│   [0m                                                                                                                                                             [31m│[0m
[31m│[0m   [2m37 [0m[2m│   [0mtrain_dataloader = torch.utils.data.DataLoader(train_set, batch_size=hparams[[33m'[0m[33mbatch_[0m                                                                         [31m│[0m
[31m│[0m   [2m38 [0m                                                                                                                                                                 [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m [33m╭─[0m[33m────────────────────────────────────────────────────────────────────────────[0m[33m locals [0m[33m─────────────────────────────────────────────────────────────────────────────[0m[33m─╮[0m [31m│[0m
[31m│[0m [33m│[0m       cfg = [1m{[0m[33m'optimizer'[0m: [1m{[0m[33m'_target_'[0m: [33m'torch.optim.Adam'[0m, [33m'lr'[0m: [94m0.001[0m, [33m'betas'[0m: [1m[[0m[94m0.9[0m, [94m0.999[0m[1m][0m, [33m'eps'[0m: [94m1e-08[0m, [33m'weight_decay'[0m: [94m0[0m[1m}[0m, [33m'training'[0m: [1m{[0m[33m'model_path'[0m:       [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [33m'models/model.pth'[0m, [33m'fig_path'[0m: [33m'reports/figures/training_statistics.png'[0m, [33m'batch_size'[0m: [94m32[0m, [33m'epochs'[0m: [94m10[0m[1m}[0m[1m}[0m                                           [33m│[0m [31m│[0m
[31m│[0m [33m│[0m    DEVICE = [1;35mdevice[0m[1m([0m[33mtype[0m=[33m'cpu'[0m[1m)[0m                                                                                                                                    [33m│[0m [31m│[0m
[31m│[0m [33m│[0m   hparams = [1m{[0m[33m'model_path'[0m: [33m'models/model.pth'[0m, [33m'fig_path'[0m: [33m'reports/figures/training_statistics.png'[0m, [33m'batch_size'[0m: [94m32[0m, [33m'epochs'[0m: [94m10[0m[1m}[0m                             [33m│[0m [31m│[0m
[31m│[0m [33m│[0m     model = [1;35mMyAwesomeModel[0m[1m([0m                                                                                                                                       [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m  [0m[1m([0mconv1[1m)[0m: [1;35mConv2d[0m[1m([0m[94m1[0m, [94m32[0m, [33mkernel_size[0m=[1m([0m[94m3[0m, [94m3[0m[1m)[0m, [33mstride[0m=[1m([0m[94m1[0m, [94m1[0m[1m)[0m[1m)[0m                                                                                           [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m  [0m[1m([0mconv2[1m)[0m: [1;35mConv2d[0m[1m([0m[94m32[0m, [94m64[0m, [33mkernel_size[0m=[1m([0m[94m3[0m, [94m3[0m[1m)[0m, [33mstride[0m=[1m([0m[94m1[0m, [94m1[0m[1m)[0m[1m)[0m                                                                                          [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m  [0m[1m([0mconv3[1m)[0m: [1;35mConv2d[0m[1m([0m[94m64[0m, [94m128[0m, [33mkernel_size[0m=[1m([0m[94m3[0m, [94m3[0m[1m)[0m, [33mstride[0m=[1m([0m[94m1[0m, [94m1[0m[1m)[0m[1m)[0m                                                                                         [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m  [0m[1m([0mdropout[1m)[0m: [1;35mDropout[0m[1m([0m[33mp[0m=[94m0[0m[94m.5[0m, [33minplace[0m=[94mFalse[0m[1m)[0m                                                                                                            [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m  [0m[1m([0mfc1[1m)[0m: [1;35mLinear[0m[1m([0m[33min_features[0m=[94m128[0m, [33mout_features[0m=[94m10[0m, [33mbias[0m=[94mTrue[0m[1m)[0m                                                                                          [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [1m)[0m                                                                                                                                                     [33m│[0m [31m│[0m
[31m│[0m [33m│[0m optimizer = Adam [1m([0m                                                                                                                                                [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             Parameter Group [94m0[0m                                                                                                                                     [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0mamsgrad: [94mFalse[0m                                                                                                                                    [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0mbetas: [1m[[0m[94m0.9[0m, [94m0.999[0m[1m][0m                                                                                                                               [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0mcapturable: [94mFalse[0m                                                                                                                                 [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0mdifferentiable: [94mFalse[0m                                                                                                                             [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0meps: [94m1e-08[0m                                                                                                                                        [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0mforeach: [94mNone[0m                                                                                                                                     [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0mfused: [94mNone[0m                                                                                                                                       [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0mlr: [94m0.001[0m                                                                                                                                         [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0mmaximize: [94mFalse[0m                                                                                                                                   [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [2m│   [0mweight_decay: [94m0[0m                                                                                                                                   [33m│[0m [31m│[0m
[31m│[0m [33m│[0m             [1m)[0m                                                                                                                                                     [33m│[0m [31m│[0m
[31m│[0m [33m╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m [33mC:\Users\jonas\Desktop\DTU\MLOps\my_mlops\src\session2\data.py[0m:[94m38[0m in [92mcorrupt_mnist[0m                                                                                    [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m   [2m35 [0m                                                                                                                                                                 [31m│[0m
[31m│[0m   [2m36 [0m[94mdef[0m[90m [0m[92mcorrupt_mnist[0m() -> [96mtuple[0m[torch.utils.data.Dataset, torch.utils.data.Dataset]:                                                                                [31m│[0m
[31m│[0m   [2m37 [0m[2;90m│   [0m[33m"""Return train and test datasets for corrupt MNIST."""[0m                                                                                                      [31m│[0m
[31m│[0m [31m❱ [0m38 [2m│   [0mtrain_images = [1;4mtorch.load([0m[1;4;33m"[0m[1;4;33mdata/processed/train_images.pt[0m[1;4;33m"[0m[1;4m)[0m                                                                                                  [31m│[0m
[31m│[0m   [2m39 [0m[2m│   [0mtrain_target = torch.load([33m"[0m[33mdata/processed/train_target.pt[0m[33m"[0m)                                                                                                  [31m│[0m
[31m│[0m   [2m40 [0m[2m│   [0mtest_images = torch.load([33m"[0m[33mdata/processed/test_images.pt[0m[33m"[0m)                                                                                                    [31m│[0m
[31m│[0m   [2m41 [0m[2m│   [0mtest_target = torch.load([33m"[0m[33mdata/processed/test_target.pt[0m[33m"[0m)                                                                                                    [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m [33mC:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\torch\serialization.py[0m:[94m1319[0m in [92mload[0m                                                                           [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m   [2m1316 [0m[2m│   [0m[94mif[0m [33m"[0m[33mencoding[0m[33m"[0m [95mnot[0m [95min[0m pickle_load_args.keys():                                                                                                              [31m│[0m
[31m│[0m   [2m1317 [0m[2m│   │   [0mpickle_load_args[[33m"[0m[33mencoding[0m[33m"[0m] = [33m"[0m[33mutf-8[0m[33m"[0m                                                                                                                 [31m│[0m
[31m│[0m   [2m1318 [0m[2m│   [0m                                                                                                                                                           [31m│[0m
[31m│[0m [31m❱ [0m1319 [2m│   [0m[94mwith[0m [1;4m_open_file_like(f, [0m[1;4;33m"[0m[1;4;33mrb[0m[1;4;33m"[0m[1;4m)[0m [94mas[0m opened_file:                                                                                                              [31m│[0m
[31m│[0m   [2m1320 [0m[2m│   │   [0m[94mif[0m _is_zipfile(opened_file):                                                                                                                           [31m│[0m
[31m│[0m   [2m1321 [0m[2m│   │   │   [0m[2m# The zipfile reader is going to advance the current file position.[0m                                                                                [31m│[0m
[31m│[0m   [2m1322 [0m[2m│   │   │   [0m[2m# If we want to actually tail call to torch.jit.load, we need to[0m                                                                                   [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m [33m╭─[0m[33m─────────────────────────────────────────────────────────[0m[33m locals [0m[33m─────────────────────────────────────────────────────────[0m[33m─╮[0m                                        [31m│[0m
[31m│[0m [33m│[0m      DOCS_MESSAGE = [33m'\n\nCheck the documentation of torch.load to learn more about types accepted by de'[0m+[94m82[0m                [33m│[0m                                        [31m│[0m
[31m│[0m [33m│[0m                 f = [33m'data/processed/train_images.pt'[0m                                                                       [33m│[0m                                        [31m│[0m
[31m│[0m [33m│[0m      map_location = [94mNone[0m                                                                                                   [33m│[0m                                        [31m│[0m
[31m│[0m [33m│[0m              mmap = [94mFalse[0m                                                                                                  [33m│[0m                                        [31m│[0m
[31m│[0m [33m│[0m  pickle_load_args = [1m{[0m[33m'encoding'[0m: [33m'utf-8'[0m[1m}[0m                                                                                  [33m│[0m                                        [31m│[0m
[31m│[0m [33m│[0m     pickle_module = [1m<[0m[1;95mmodule[0m[39m [0m[33m'pickle'[0m[39m from [0m[33m'C:\\Users\\jonas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\pickle.py'[0m[1m>[0m [33m│[0m                                        [31m│[0m
[31m│[0m [33m│[0m         skip_data = [94mFalse[0m                                                                                                  [33m│[0m                                        [31m│[0m
[31m│[0m [33m│[0m    UNSAFE_MESSAGE = [33m'Re-running `torch.load` with `weights_only` set to `False` will likely succeed, '[0m+[94m100[0m                 [33m│[0m                                        [31m│[0m
[31m│[0m [33m│[0m warn_weights_only = [94mTrue[0m                                                                                                   [33m│[0m                                        [31m│[0m
[31m│[0m [33m│[0m      weights_only = [94mFalse[0m                                                                                                  [33m│[0m                                        [31m│[0m
[31m│[0m [33m╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m                                        [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m [33mC:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\torch\serialization.py[0m:[94m659[0m in [92m_open_file_like[0m                                                                 [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m   [2m 656 [0m                                                                                         [33m╭─[0m[33m────────────────────[0m[33m locals [0m[33m─────────────────────[0m[33m─╮[0m                 [31m│[0m
[31m│[0m   [2m 657 [0m[94mdef[0m[90m [0m[92m_open_file_like[0m(name_or_buffer, mode):                                               [33m│[0m           mode = [33m'rb'[0m                             [33m│[0m                 [31m│[0m
[31m│[0m   [2m 658 [0m[2m│   [0m[94mif[0m _is_path(name_or_buffer):                                                         [33m│[0m name_or_buffer = [33m'data/processed/train_images.pt'[0m [33m│[0m                 [31m│[0m
[31m│[0m [31m❱ [0m 659 [2m│   │   [0m[94mreturn[0m [1;4m_open_file(name_or_buffer, mode)[0m                                          [33m╰───────────────────────────────────────────────────╯[0m                 [31m│[0m
[31m│[0m   [2m 660 [0m[2m│   [0m[94melse[0m:                                                                                                                                                      [31m│[0m
[31m│[0m   [2m 661 [0m[2m│   │   [0m[94mif[0m [33m"[0m[33mw[0m[33m"[0m [95min[0m mode:                                                                                                                                        [31m│[0m
[31m│[0m   [2m 662 [0m[2m│   │   │   [0m[94mreturn[0m _open_buffer_writer(name_or_buffer)                                                                                                         [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m [33mC:\Users\jonas\Desktop\Envs\.venv_ml1\Lib\site-packages\torch\serialization.py[0m:[94m640[0m in [92m__init__[0m                                                                        [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m   [2m 637 [0m                                                                                                                                                               [31m│[0m
[31m│[0m   [2m 638 [0m[94mclass[0m[90m [0m[4;92m_open_file[0m(_opener):                                                                                                                                     [31m│[0m
[31m│[0m   [2m 639 [0m[2m│   [0m[94mdef[0m[90m [0m[92m__init__[0m([96mself[0m, name, mode):                                                                                                                            [31m│[0m
[31m│[0m [31m❱ [0m 640 [2m│   │   [0m[96msuper[0m().[92m__init__[0m([1;4;96mopen[0m[1;4m(name, mode)[0m)                                                                                                                     [31m│[0m
[31m│[0m   [2m 641 [0m[2m│   [0m                                                                                                                                                           [31m│[0m
[31m│[0m   [2m 642 [0m[2m│   [0m[94mdef[0m[90m [0m[92m__exit__[0m([96mself[0m, *args):                                                                                                                                 [31m│[0m
[31m│[0m   [2m 643 [0m[2m│   │   [0m[96mself[0m.file_like.close()                                                                                                                                 [31m│[0m
[31m│[0m                                                                                                                                                                       [31m│[0m
[31m│[0m [33m╭─[0m[33m──────────────────────────────[0m[33m locals [0m[33m──────────────────────────────[0m[33m─╮[0m                                                                                              [31m│[0m
[31m│[0m [33m│[0m mode = [33m'rb'[0m                                                          [33m│[0m                                                                                              [31m│[0m
[31m│[0m [33m│[0m name = [33m'data/processed/train_images.pt'[0m                              [33m│[0m                                                                                              [31m│[0m
[31m│[0m [33m│[0m self = [1m<[0m[1;95mtorch.serialization._open_file[0m[39m object at [0m[94m0x000002604CC4C690[0m[1m>[0m [33m│[0m                                                                                              [31m│[0m
[31m│[0m [33m╰──────────────────────────────────────────────────────────────────────╯[0m                                                                                              [31m│[0m
[31m╰───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯[0m
[1;91mFileNotFoundError: [0m[1m[[0mErrno [1;36m2[0m[1m][0m No such file or directory: [32m'data/processed/train_images.pt'[0m
